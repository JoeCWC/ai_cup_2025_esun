import pandas as pd
import numpy as np
import os
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from sklearn.metrics import classification_report,roc_auc_score
from sklearn.metrics import f1_score, precision_score, recall_score
import joblib

# =========================
# 0. è¨­å®š input æª”æ¡ˆè·¯å¾‘
# =========================
acct_transaction_csv =  "/home/joe/ai_cup_2025/bank/data_set/acct_transaction.csv"
acct_alert_csv =        "/home/joe/ai_cup_2025/bank/data_set/acct_alert.csv"
acct_predict_csv =      "/home/joe/ai_cup_2025/bank/data_set/acct_predict.csv"

# =========================
# 0. è¨­å®š output æª”æ¡ˆè·¯å¾‘
# =========================
acct_predict_result_csv =   "/home/joe/ai_cup_2025/bank/trained_model_and_prediction/acct_predict_result.csv"
saved_model_path =          "/home/joe/ai_cup_2025/bank/trained_model_and_prediction/xgb_acctlevel_model.joblib"

# =========================
# 0. æª¢æŸ¥ input æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±é€€å‡º
# =========================
for path in [acct_transaction_csv, acct_alert_csv, acct_predict_csv]:
    if not os.path.exists(path):
        print(f"[ERROR] æ‰¾ä¸åˆ°å¿…è¦çš„è¼¸å…¥æª”æ¡ˆ: {path}")
        sys.exit(1)  # ç›´æ¥é€€å‡ºç¨‹å¼

# =========================
# 0. æª¢æŸ¥ output ç›®éŒ„æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å»ºç«‹
# =========================
for path in [acct_predict_result_csv, saved_model_path]:
    out_dir = os.path.dirname(path)
    if not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)
        print(f"[INFO] å»ºç«‹ç›®éŒ„: {out_dir}")

# =========================
# 1. è¼‰å…¥è³‡æ–™
# =========================
print("[INFO] è¼‰å…¥è³‡æ–™...")
txn_df = pd.read_csv(acct_transaction_csv)
alert_df = pd.read_csv(acct_alert_csv)

# =========================
# 2. å»ºç«‹å¸³æˆ¶å±¤ç´šç‰¹å¾µ

# ã€Œå¸³æˆ¶å±¤ç´šç‰¹å¾µè¡¨ã€
# - æ™‚é–“ç‰¹å¾µï¼ˆäº¤æ˜“é »ç‡ã€å¤œé–“æ¯”ä¾‹ï¼‰
# - é‡‘é¡ç‰¹å¾µï¼ˆå¹³å‡ã€æœ€å¤§ã€æ¨™æº–å·®ï¼‰
# - ç¶²è·¯ç‰¹å¾µï¼ˆå‡ºåº¦ã€å…¥åº¦ã€å°æ‰‹æ•¸é‡ï¼‰

# é€é æ™‚é–“ç·šæ‹†è§£ + é‡‘é¡åˆ†æ + äº¤æ˜“å°æ‰‹ç¶²è·¯ï¼Œå»æ‰¾å‡ºã€Œæœªä¾†å¯èƒ½è¢«åˆ¤å®šç‚ºè­¦ç¤ºã€çš„å¸³æˆ¶ã€‚é€™å€‹éœ€æ±‚å…¶å¯¦å°±æ˜¯ åæ´—éŒ¢ (AML) / å¯ç–‘äº¤æ˜“åµæ¸¬ çš„å…¸å‹ä»»å‹™ã€‚

# æ™‚é–“ç·šæ‹†è§£ (Temporal Features)
# - äº¤æ˜“é »ç‡ï¼šè¨ˆç®—å¸³æˆ¶åœ¨ä¸åŒæ™‚é–“çª—å…§çš„äº¤æ˜“æ¬¡æ•¸ï¼ˆè¿‘ 1 å¤©ã€7 å¤©ã€30 å¤©ï¼‰ã€‚
# - äº¤æ˜“é–“éš”ï¼šå¹³å‡äº¤æ˜“é–“éš”æ™‚é–“ã€æœ€çŸ­/æœ€é•·é–“éš”ã€‚
# - æ™‚é–“åˆ†å¸ƒï¼šå¤œé–“äº¤æ˜“æ¯”ä¾‹ï¼ˆ22:00â€“06:00ï¼‰ã€å°–å³°æ™‚æ®µäº¤æ˜“æ¯”ä¾‹ã€‚
# - ç•°å¸¸æ¨¡å¼ï¼šçŸ­æ™‚é–“å…§å¤šç­†å¤§é¡äº¤æ˜“ã€é€£çºŒè½‰å¸³åˆ°ä¸åŒå¸³æˆ¶ã€‚
# ğŸ‘‰ é€™äº›ç‰¹å¾µèƒ½æ•æ‰ã€Œæ´—éŒ¢å¸¸è¦‹çš„æ™‚é–“ç•°å¸¸è¡Œç‚ºã€

# é‡‘é¡åˆ†æ (Amount Features)
# - çµ±è¨ˆç‰¹å¾µï¼šå¹³å‡é‡‘é¡ã€æœ€å¤§é‡‘é¡ã€æœ€å°é‡‘é¡ã€æ¨™æº–å·®ã€‚
# - å¤§é¡æ¯”ä¾‹ï¼šå¤§æ–¼æŸé–€æª»ï¼ˆä¾‹å¦‚ 50 è¬ï¼‰çš„äº¤æ˜“æ¯”ä¾‹ã€‚
# - é‡‘é¡åˆ†å¸ƒï¼šå°é¡é«˜é » vs. å¤§é¡ä½é »ã€‚
# - å¹£åˆ¥ç•°å¸¸ï¼šæ˜¯å¦é »ç¹ä½¿ç”¨å¤–å¹£ï¼ˆUSDã€JPYâ€¦ï¼‰è½‰å¸³ã€‚
# ğŸ‘‰ é€™äº›ç‰¹å¾µèƒ½æ•æ‰ã€Œå°é¡åˆ‡å‰² (structuring)ã€æˆ–ã€Œå¤§é¡ç•°å¸¸ã€çš„æ¨¡å¼

# äº¤æ˜“å°æ‰‹ç¶²è·¯ (Graph Features)
# - å‡ºåº¦ (out-degree)ï¼šè©²å¸³æˆ¶è½‰å‡ºçµ¦å¤šå°‘ä¸åŒå¸³æˆ¶ã€‚
# - å…¥åº¦ (in-degree)ï¼šè©²å¸³æˆ¶æ”¶éå¤šå°‘ä¸åŒå¸³æˆ¶ã€‚
# - é›™å‘äº¤æ˜“æ¯”ä¾‹ï¼šæ˜¯å¦å­˜åœ¨ã€Œäº’ç›¸è½‰å¸³ã€çš„é—œä¿‚ã€‚
# - èˆ‡è­¦ç¤ºå¸³æˆ¶çš„é—œè¯ï¼šæ˜¯å¦ç›´æ¥æˆ–é–“æ¥èˆ‡å·²çŸ¥è­¦ç¤ºå¸³æˆ¶æœ‰äº¤æ˜“ã€‚
# - ç¤¾ç¾¤ç‰¹å¾µï¼šé€éåœ–æ¼”ç®—æ³•ï¼ˆPageRankã€Connected Componentsï¼‰æ‰¾å‡ºå¯ç–‘é›†åœ˜ã€‚
# ğŸ‘‰ é€™äº›ç‰¹å¾µèƒ½æ•æ‰ã€Œäººé ­å¸³æˆ¶é›†åœ˜ã€æˆ–ã€Œè³‡é‡‘æ´—ç™½ç¶²è·¯ã€

# ä¸åˆç†äº¤æ˜“æ¨¡å¼çš„åˆ¤æ–·é‚è¼¯
# é™¤äº†æ¨¡å‹é æ¸¬ï¼Œä½ ä¹Ÿå¯ä»¥è¨­è¨ˆ è¦å‰‡æª¢æ¸¬ (rule-based features)ï¼Œä¾‹å¦‚ï¼š
# - å–®æ—¥äº¤æ˜“æ¬¡æ•¸ > 50 ä¸”é‡‘é¡ç¸½å’Œ > 100 è¬ã€‚
# - å¤œé–“äº¤æ˜“æ¯”ä¾‹ > 80%ã€‚
# - å‡ºåº¦ > 20 ä¸”äº¤æ˜“å°æ‰‹å¤šç‚ºæ–°å¸³æˆ¶ã€‚
# - èˆ‡å·²çŸ¥è­¦ç¤ºå¸³æˆ¶æœ‰ç›´æ¥äº¤æ˜“ã€‚
# é€™äº›è¦å‰‡å¯ä»¥å’Œ XGBoost æ¨¡å‹çµåˆï¼Œå½¢æˆ Hybrid Systemï¼Œæå‡å¯è§£é‡‹æ€§ã€‚
# =========================
print("[INFO] å»ºç«‹å¸³æˆ¶å±¤ç´šç‰¹å¾µ...")

# (a) é‡‘é¡ç‰¹å¾µ
amt_stats = txn_df.groupby("from_acct")["txn_amt"].agg(
    txn_amt_mean="mean",
    txn_amt_max="max",
    txn_amt_std="std",
    txn_count="count"
).reset_index().rename(columns={"from_acct": "acct"})

# (b) æ™‚é–“ç‰¹å¾µ
txn_df["txn_hour"] = pd.to_datetime(txn_df["txn_time"], format="%H:%M:%S", errors="coerce").dt.hour
txn_df["is_night"] = txn_df["txn_hour"].apply(lambda h: 1 if pd.notnull(h) and (h < 6 or h >= 22) else 0)

time_stats = txn_df.groupby("from_acct").agg(
    night_ratio=("is_night", "mean"),
    txn_per_day=("txn_date", lambda x: len(x) / (x.max() - x.min() + 1))
).reset_index().rename(columns={"from_acct": "acct"})

# (c) ç¶²è·¯ç‰¹å¾µ
out_degree = txn_df.groupby("from_acct")["to_acct"].nunique().reset_index()
out_degree = out_degree.rename(columns={"from_acct": "acct", "to_acct": "out_degree"})

in_degree = txn_df.groupby("to_acct")["from_acct"].nunique().reset_index()
in_degree = in_degree.rename(columns={"to_acct": "acct", "from_acct": "in_degree"})

# (d) åˆä½µæ‰€æœ‰ç‰¹å¾µ
acct_features = amt_stats.merge(time_stats, on="acct", how="left")
acct_features = acct_features.merge(out_degree, on="acct", how="left")
acct_features = acct_features.merge(in_degree, on="acct", how="left")

# ç¼ºå¤±å€¼è£œ 0
acct_features = acct_features.fillna(0)

# =========================
# 3. å»ºç«‹æ¨™ç±¤
# =========================
print("[INFO] å»ºç«‹æ¨™ç±¤...")
alert_df["label"] = 1
acct_features = acct_features.merge(alert_df[["acct", "label"]], on="acct", how="left")
acct_features["label"] = acct_features["label"].fillna(0).astype(int)

# =========================
# 4. åˆ‡åˆ†è³‡æ–™
# =========================
print("[INFO] åˆ‡åˆ†è³‡æ–™...")
X = acct_features.drop(columns=["acct", "label"])
y = acct_features["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# =========================
# 5. å»ºç«‹ pipeline
# =========================
print("[INFO] å»ºç«‹å‰è™•ç† pipeline...")
numeric_features = X.columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ("num", Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler())
        ]), numeric_features)
    ]
)

pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor)
])

# =========================
# 6. å‰è™•ç†
# =========================
print("[INFO] å‰è™•ç†è³‡æ–™...")
X_train_processed = pipeline.fit_transform(X_train)
X_test_processed = pipeline.transform(X_test)


# =========================
# 7. è™•ç†æ¨£æœ¬ä¸å¹³è¡¡ï¼šscale_pos_weight
#   è¨­ç‚º (è² æ¨£æœ¬æ•¸ / æ­£æ¨£æœ¬æ•¸)
# =========================
neg, pos = np.bincount(y_train)
scale_pos_weight = (neg / pos) if pos > 0 else 1.0
print(f"æ­£æ¨£æœ¬æ•¸={pos}, è² æ¨£æœ¬æ•¸={neg}, scale_pos_weight={scale_pos_weight:.2f}")


# =========================
# 8. è¨“ç·´ XGBoost
# =========================
model = XGBClassifier(
    scale_pos_weight=scale_pos_weight,  # é¡åˆ¥ä¸å¹³è¡¡è™•ç†ï¼Œé€šå¸¸è¨­ç‚º neg/posï¼Œè®“å°‘æ•¸é¡åˆ¥æ¬Šé‡æ›´é«˜
    max_depth=5,                        # æ¯æ£µæ¨¹çš„æœ€å¤§æ·±åº¦ï¼Œæ§åˆ¶æ¨¡å‹è¤‡é›œåº¦ï¼Œéå¤§å®¹æ˜“ overfittingï¼Œéå°å¯èƒ½ underfittingï¼Œå¸¸è¦‹ç¯„åœï¼š3â€“8
    learning_rate=0.02,                 # å­¸ç¿’ç‡ (shrinkage)ï¼Œæ­¥ä¼å°ï¼Œæ”¶æ–‚æ…¢ä½†æ›´ç©©å®šï¼Œä½†éœ€è¦æ›´å¤šæ¨¹ (n_estimators)
                                        # é€šå¸¸å›ºå®šä¸€å€‹è¼ƒå°çš„ learning_rateï¼ˆå¦‚ 0.05ï¼‰ï¼Œå†ç”¨ early_stopping_rounds è‡ªå‹•æŒ‘é¸æœ€ä½³æ¨¹æ•¸ã€‚
    n_estimators=5000,                  # æœ€å¤§æ¨¹æ•¸ (boosting rounds)ï¼Œæ­é… early stopping
    subsample=0.9,                      # æ§åˆ¶æ¯æ£µæ¨¹ç”¨å¤šå°‘æ¨£æœ¬ï¼Œ<1.0 æ™‚èƒ½å¢åŠ éš¨æ©Ÿæ€§ï¼Œé™ä½éæ“¬åˆ
    colsample_bytree=0.9,               # æ§åˆ¶æ¯æ£µæ¨¹ç”¨å¤šå°‘ç‰¹å¾µï¼Œå’Œ subsample æ­é…èª¿æ•´ï¼Œå¸¸è¦‹çµ„åˆï¼š0.7â€“0.9
    reg_lambda=1.5,                     # L2 æ­£å‰‡åŒ– (Ridge)ï¼ŒæŠ‘åˆ¶æ¬Šé‡éå¤§ï¼Œæå‡æ¨¡å‹ç©©å®šæ€§
    reg_alpha=0.3,                      # L1 æ­£å‰‡åŒ– (Lasso)ï¼Œé¼“å‹µç¨€ç–æ€§ï¼Œæœ‰åŠ©æ–¼ç‰¹å¾µé¸æ“‡
    early_stopping_rounds=100,          # çµ¦æ¨¡å‹å……è¶³æ™‚é–“åœæ­¢ è¨­ç‚º None å¯é—œé–‰ early stoppingï¼Œå®Œæ•´è§€å¯Ÿ learning curve
    eval_metric="logloss",              # è©•ä¼°æŒ‡æ¨™ï¼ˆloglossã€aucã€aucprï¼‰ï¼Œå°æ¥µåº¦ä¸å¹³è¡¡çš„è³‡æ–™ï¼Œaucpr é€šå¸¸æ¯” logloss æ›´æ•æ„Ÿ
    objective="binary:logistic",        # äºŒå…ƒåˆ†é¡
    #tree_method="gpu_hist",            # å¦‚æœè³‡æ–™é‡å¤§ï¼Œ tree_method="hist"ï¼Œèƒ½é¡¯è‘—åŠ é€Ÿè¨“ç·´ï¼›å¦‚æœè³‡æ–™é‡å°ï¼Œä¸åŠ ä¹Ÿæ²’å·®ï¼Œauto æœƒè‡ªå‹•é¸æ“‡
    device="cuda",                      # å¦‚æœæœ‰ GPU ï¼Œåœç”¨tree_methodï¼Œæ”¹ç”¨ device="cuda"
    random_state=42,                    # å›ºå®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
    n_jobs=-1,                          # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒåŠ é€Ÿè¨“ç·´
    verbosity=1                         # é¡¯ç¤ºè¨“ç·´è³‡è¨Š
)

model.fit(
    X_train_processed, y_train,
    eval_set=[(X_train_processed, y_train), (X_test_processed, y_test)],
    verbose=True
    )

# =========================
# 9. è©•ä¼°
# =========================
y_pred = model.predict(X_test_processed)
y_proba = model.predict_proba(X_test_processed)[:, 1]

print("\nClassification report:")
print(classification_report(y_test, y_pred, digits=4))

try:
    auc = roc_auc_score(y_test, y_proba) # è¨ˆç®— ROC æ›²ç·šä¸‹é¢ç©ï¼Œè¡¡é‡åˆ†é¡å™¨çš„å€åˆ†èƒ½åŠ›
    print(f"AUC: {auc:.4f}")
except ValueError:
    print("AUC ç„¡æ³•è¨ˆç®—ï¼ˆå¯èƒ½å…¨ç‚ºå–®ä¸€é¡åˆ¥ï¼‰")

# =========================
# 10. Evaluate multiple thresholds
# =========================
thresholds = np.linspace(0.1, 0.9, 17)  # (èµ·å§‹, çµæŸ, å€‹æ•¸)
# åˆå§‹åŒ–æœ€ä½³é–¾å€¼èˆ‡æœ€ä½³ F1 åˆ†æ•¸
best_threshold = 0.5 
best_f1 = 0

print("\nThreshold evaluation:")
for t in thresholds:
    y_pred_thresh = (y_proba >= t).astype(int)
    f1 = f1_score(y_test, y_pred_thresh)
    precision = precision_score(y_test, y_pred_thresh, zero_division=0)
    recall = recall_score(y_test, y_pred_thresh, zero_division=0)
    print(f"Threshold={t:.2f} | F1={f1:.4f} | Precision={precision:.4f} | Recall={recall:.4f}")
    
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

print(f"\næœ€ä½³ Threshold: {best_threshold:.2f}")
print(f"å°æ‡‰çš„ F1: {best_f1:.4f}\n")


# =========================
# 11. å„²å­˜æ¨¡å‹
# =========================
joblib.dump(
    {
        "pipeline": pipeline,
        "model": model,
        "best_threshold": best_threshold
    },
    saved_model_path
    )

print(f"[INFO] å¸³æˆ¶å±¤ç´šæ¨¡å‹å·²å­˜æª”: {saved_model_path}")


# =========================
# 12. è¼‰å…¥æ¨¡å‹
# =========================
print("[INFO] è¼‰å…¥æ¨¡å‹é€²è¡Œé æ¸¬...")
saved = joblib.load(saved_model_path)
pipeline = saved["pipeline"]
model = saved["model"]

# =========================
# 13. è¼‰å…¥ acct_predict.csv
# =========================
predict_df = pd.read_csv(acct_predict_csv)

# =========================
# 14. åˆä½µå¸³æˆ¶ç‰¹å¾µ
# =========================
predict_df = predict_df.merge(acct_features, on="acct", how="left").fillna(0)

X_pred = predict_df.drop(columns=["acct", "label"], errors="ignore")
X_pred_processed = pipeline.transform(X_pred)

# =========================
# 15. é æ¸¬
# =========================
y_pred = model.predict(X_pred_processed)

result_df = pd.DataFrame({
    "acct": predict_df["acct"],
    "label": y_pred
})

#print(result_df.head())
result_df.to_csv(acct_predict_result_csv, index=False)
print(f"[INFO] é æ¸¬çµæœå·²è¼¸å‡ºåˆ° {acct_predict_result_csv}")